---
title: "Homework-3"
author: "Qi Tan"
output: html_document
---

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The Homework3 vignette}
-->

## 1. (P117-7) Write a function kern_density that accepts a training vector x, bandwidth h, and test set x_new, returning the kernel density estimate from the Epanechnikov kernel. Visually test how this performs for some hand constructed datasets and bandwidths.
Epanechnikov kernel (4.12): $K(x)=\frac{3}{4}(1-x^2)1_{|x|\leq 1}$\
Density (4.63): $f_h(x)=\frac{1}{n}\sum_iK_h(x-x_i)$
```{r}
#Epanechnikov kernekl function (P81)
casl_util_kernel_epan <- function(x, h=1) {
  x <- x/h
  ran <- as.numeric(abs(x) <= 1)
  val <- (3/4) * ( 1 - x^2 ) * ran
  val
}

#Kernel density
kern_density <- function(x, h, x_new) {
  sapply(x_new, function(v)
    {
    w <- casl_util_kernel_epan(abs(x-v), h=h)
    density <- mean(w)
    density
    })
}

#Create datasets
N=100
set.seed(1)
x <- rnorm(N)
x_new <- rnorm(N)
bwidths <- c(0.01, 0.1, 0.5, 0.75, 1, 1.5)

#Plot
par(mfrow=c(1, 3))
for (i in bwidths) {
  plot(x_new, kern_density(x=x, h=i, x_new=x_new), xlab='Observed', ylab='Kernel', main='Kernel Density')
}
par(mfrow=c(1, 1))
```
As bandwidths increases, the curve is closer to normal distribution.


## 2. (P200-3) Show that if f and g are both convex functions, then their sum must also be convex.
Given f and g are both convex functions:
According to definition of convex (7.25), for any values $b_1$, $b_2 \in R^P$ and quantity $t \in [0,1]$, we have 
$$f(tb_1+(1-t)b_2)\leq tf(b_1)+(1-t)f(b_2)$$
$$g(tb_1+(1-t)b_2)\leq tg(b_1)+(1-t)g(b_2)$$
\
Let $l=f+g$, then
$$l(tb_1+(1-t)b_2)=f(tb_1+(1-t)b_2)+g(tb_1+(1-t)b_2)\leq tf(b_1)+(1-t)f(b_2)+tg(b_1)+(1-t)g(b_2)\\
=t(f(b_1)+g(b_1))+(1-t)(f(b_2)+g(b_2))=tl(b_1)+(1-t)l(b_2)$$
As a result, l is also a convex function.


## 3. (P200-4) Illustrate that the absolute value function is convex. Using the result from the previous exercise, show that the l1-norm is also convex.
Let $f(x)=|x|$, $t \in [0,1]$, and $b_1$, $b_2 \in R^P$
$$f(tb_1+(1-t)b_2)=|tb_1+(1-t)b_2| \leq t|b_1|+(1-t)|b_2|=tf(b_1)+(1-t)f(b_2)$$
According to definition of convex (7.25), $f(x)=|x|$ is a convex function.\
\
According to definition of $l_1$-norm (7.3): $$||v||_1=\sum_j|v_j|$$
Since $f(x)=|x|$ is convex, then $\sum_jf(v_j)$ is also convex (previous question).\
As a result, $l_1$-norm: $||v||_1$ is convex.


## 4. (P200-5) Prove that the elastic net objective function is convex using the results from the previous two exercises.
Let $f(x)=|x|$ and $g(x)=(x)^2$\
According to previous function, $f(x)$ is convex.\
$$g(tb_1+(1-t)b_2)=(tb_1+(1-t)b_2)^2=t^2b_1^2+(1-t)^2b_2^2+2t(1-t)b_1b_2$$
$$tg(b_1)+(1-t)g(b_2)=tb_1^2+(1-t)b_2^2$$
$$tg(b_1)+(1-t)g(b_2)-g(tb_1+(1-t)b_2)=t(1-t)b_1^2+t(1-t)b_2^2-2t(1-t)b_1b_2\\
=t(1-t)[b_1^2+b_2^2-2b_1b_2]=t(1-t)(b_1-b_2)^2 \geq0$$
Thus, $g(x)=(x)^2$ is convex.\
\
Let $h(x)=\beta k(x)$ for $\beta>0$ adn $k(x)$ is convex.\
$$h(tb_1+(1-t)b_2)=\beta k(tb_1+(1-t)b_2)\\
\leq \beta (tk(b_1)+(1-t)k(b_2))=t(\beta k(b_1))+(1-t)(\beta k(b_2))=th(b_1)+(1-t)h(b_2)$$
Thus $h(x)=\beta k(x)$ is also convex for $\beta>0$.\
\
Since elastic net$=\frac{1}{2n}||y-Xb||_2^2+\lambda((1-\alpha)\frac{1}{2}||b||_2^2+\alpha||b||_1)$\
Given $\lambda>0$ and $\alpha \in[0,1]$,\
then $\frac{1}{2n}>0$ and $\lambda(1-\alpha)\frac{1}{2}>0$, and $\lambda\alpha>0$.\
Then elastic net is convex because sum of convex functions is convex.


## 5. (P200-6) Find the KKT conditions for glmnet when 0 < α ≤ 1 and implement a lasso_reg_with_screening function that takes an α parameter.
```{r}
#require(glmnet)
#Check current KKT conditions for regression vector (P189)
casl_lenet_check_kkt <- function(X, y, b, lambda) {
  resids <- y - X %*% b
  s <- apply(X, 2, function(xj) crossprod(xj, resids)) / lambda / nrow(X)
  #Return a vector indicating where the KKT conditions have been violated by the variables that are currently zero.
  (b == 0) & (abs(s) >= 1)
}

#Lasso_reg_with_screening
lasso_reg_with_screening <- function(X, y, alpha=1){
  model <- cv.glmnet(X, y, alpha=alpha)
  lambda <- model$lambda.1se
  b <- model$glmnet.fit$beta[, model$lambda==model$lambda.1se]
  casl_lenet_check_kkt(X=X, y=y, b=b, lambda=lambda)
}

#Create dataset (P190)
n <- 1000L
p <- 5000L
X <- matrix(rnorm(n * p), ncol = p)
beta <- c(seq(1, 0.1, length.out=(10L)), rep(0, p - 10L))
y <- X %*% beta + rnorm(n = n, sd = 0.15)

#No violation of KKN conditions
lasso_reg_with_screening(X, y)

#With violation of KKN conditions
model <- cv.glmnet(X, y, alpha=1)
lambda <- model$lambda.1se
b <- model$glmnet.fit$beta[, model$lambda==model$lambda.1se]
b[1] <- 0 #Force to violate
head(casl_lenet_check_kkt(X=X, y=y, b=b, lambda=lambda))
```
